"""
Complete Data Lineage Pipeline UI with Full Functionality
Interactive Streamlit application for data transformations and lineage tracking
"""

import streamlit as st
import pandas as pd
import json
import os
import sys
from typing import Dict, List, Any, Optional
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import networkx as nx
from datetime import datetime
import time

# Add project root to path
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)

# Import our modules
try:
    from src.transformations import (
        SparkTransformationEngine, 
        AVAILABLE_TRANSFORMATIONS,
        DataLineageTracker
    )
    from src.lineage import create_lineage_visualization
except ImportError as e:
    st.error(f"Failed to import modules: {e}")
    st.info("Make sure you're running from the project root directory and all dependencies are installed.")
    st.stop()

# Page configuration
st.set_page_config(
    page_title="Data Lineage Pipeline",
    page_icon="üîÑ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1e88e5;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 0.5rem;
    }
    .transformation-card {
        border: 2px solid #e0e0e0;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        background: #f9f9f9;
    }
    .transformation-card:hover {
        border-color: #1e88e5;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .success-msg {
        background: #4caf50;
        color: white;
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.5rem 0;
    }
    .error-msg {
        background: #f44336;
        color: white;
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
def initialize_session_state():
    """Initialize all session state variables"""
    if 'spark_engine' not in st.session_state:
        st.session_state.spark_engine = None
    if 'loaded_datasets' not in st.session_state:
        st.session_state.loaded_datasets = {}
    if 'transformation_history' not in st.session_state:
        st.session_state.transformation_history = []
    if 'lineage_data' not in st.session_state:
        st.session_state.lineage_data = None
    if 'pipeline_executor' not in st.session_state:
        st.session_state.pipeline_executor = None
    if 'execution_results' not in st.session_state:
        st.session_state.execution_results = []
    if 'current_step' not in st.session_state:
        st.session_state.current_step = 1

initialize_session_state()

def initialize_spark():
    """Initialize Spark engine"""
    if st.session_state.spark_engine is None:
        try:
            with st.spinner("Initializing Spark session..."):
                st.session_state.spark_engine = SparkTransformationEngine("DataLineageUI")
                st.success("‚úÖ Spark session initialized successfully!")
                return True
        except Exception as e:
            st.error(f"‚ùå Failed to initialize Spark: {e}")
            return False
    return True

def generate_fallback_data():
    """Generate minimal sample data as fallback"""
    try:
        import pandas as pd
        import numpy as np
        from faker import Faker
        import random
        
        fake = Faker()
        fake.seed(42)
        np.random.seed(42)
        random.seed(42)
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Create data directory
        data_dir = os.path.join(project_root, 'data')
        os.makedirs(data_dir, exist_ok=True)
        
        # Generate customers
        status_text.text("Generating customers data...")
        progress_bar.progress(20)
        customers_data = []
        for i in range(1000):
            customer = {
                'customer_id': f"CUST_{i+1:06d}",
                'first_name': fake.first_name(),
                'last_name': fake.last_name(),
                'email': fake.email(),
                'city': fake.city(),
                'state': fake.state(),
                'age': random.randint(18, 80),
                'registration_date': fake.date_between(start_date='-2y', end_date='today'),
                'is_active': np.random.choice([True, False], p=[0.8, 0.2])
            }
            customers_data.append(customer)
        
        customers_df = pd.DataFrame(customers_data)
        customers_df.to_csv(os.path.join(data_dir, 'customers.csv'), index=False)
        
        # Generate products
        status_text.text("Generating products data...")
        progress_bar.progress(40)
        products_data = []
        categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', 'Beauty']
        
        for i in range(200):
            product = {
                'product_id': f"PROD_{i+1:06d}",
                'product_name': fake.catch_phrase(),
                'category': random.choice(categories),
                'price': round(random.uniform(10, 500), 2),
                'cost': round(random.uniform(5, 300), 2),
                'stock_quantity': random.randint(0, 100),
                'weight': round(random.uniform(0.1, 10), 2),
                'supplier_id': f"SUPP_{random.randint(1, 50):03d}",
                'rating': round(random.uniform(1, 5), 1),
                'is_discontinued': np.random.choice([True, False], p=[0.1, 0.9])
            }
            products_data.append(product)
        
        products_df = pd.DataFrame(products_data)
        products_df.to_csv(os.path.join(data_dir, 'products.csv'), index=False)
        
        # Generate orders
        status_text.text("Generating orders data...")
        progress_bar.progress(60)
        orders_data = []
        for i in range(1000):
            order = {
                'order_id': f"ORDER_{i+1:08d}",
                'customer_id': f"CUST_{random.randint(1, 1000):06d}",
                'order_date': fake.date_between(start_date='-1y', end_date='today'),
                'total_amount': round(random.uniform(20, 1000), 2),
                'status': random.choice(['Completed', 'Pending', 'Cancelled', 'Processing']),
                'shipping_address': fake.address().replace('\n', ', '),
                'payment_method': random.choice(['Credit Card', 'PayPal', 'Bank Transfer'])
            }
            orders_data.append(order)
        
        orders_df = pd.DataFrame(orders_data)
        orders_df.to_csv(os.path.join(data_dir, 'orders.csv'), index=False)
        
        # Generate order_items
        status_text.text("Generating order items data...")
        progress_bar.progress(80)
        order_items_data = []
        for i, order in enumerate(orders_data):
            num_items = random.randint(1, 5)
            for j in range(num_items):
                item = {
                    'order_item_id': f"ITEM_{i+1:08d}_{j+1}",
                    'order_id': order['order_id'],
                    'product_id': f"PROD_{random.randint(1, 200):06d}",
                    'quantity': random.randint(1, 5),
                    'unit_price': round(random.uniform(10, 200), 2),
                    'discount': round(random.uniform(0, 0.2), 2)
                }
                order_items_data.append(item)
        
        order_items_df = pd.DataFrame(order_items_data)
        order_items_df.to_csv(os.path.join(data_dir, 'order_items.csv'), index=False)
        
        # Generate inventory
        status_text.text("Generating inventory data...")
        progress_bar.progress(90)
        inventory_data = []
        for i in range(200):
            inventory = {
                'product_id': f"PROD_{i+1:06d}",
                'warehouse_location': random.choice(['Warehouse_A', 'Warehouse_B', 'Warehouse_C', 'Warehouse_D']),
                'quantity_available': random.randint(0, 500),
                'reserved_quantity': random.randint(0, 50),
                'last_updated': fake.date_between(start_date='-30d', end_date='today'),
                'reorder_level': random.randint(10, 100)
            }
            inventory_data.append(inventory)
        
        inventory_df = pd.DataFrame(inventory_data)
        inventory_df.to_csv(os.path.join(data_dir, 'inventory.csv'), index=False)
        
        progress_bar.progress(100)
        status_text.text("Data generation completed!")
        time.sleep(1)
        progress_bar.empty()
        status_text.empty()
        
        return True
        
    except Exception as e:
        st.error(f"Failed to generate fallback data: {e}")
        return False

def load_sample_data():
    """Load sample datasets with progress tracking"""
    if not initialize_spark():
        return False
    
    data_dir = os.path.join(project_root, 'data')
    
    try:
        datasets = {}
        dataset_files = {
            'customers': 'customers.csv',
            'products': 'products.csv', 
            'orders': 'orders.csv',
            'order_items': 'order_items.csv',
            'inventory': 'inventory.csv'
        }
        
        progress_bar = st.progress(0)
        total_files = len(dataset_files)
        
        for i, (dataset_name, filename) in enumerate(dataset_files.items()):
            file_path = os.path.join(data_dir, filename)
            
            if os.path.exists(file_path):
                try:
                    df = st.session_state.spark_engine.load_data(file_path, dataset_name, 'csv')
                    datasets[dataset_name] = df
                    
                    row_count = df.count()
                    st.success(f"‚úÖ Loaded {dataset_name}: {row_count:,} records")
                    
                except Exception as e:
                    st.error(f"‚ùå Failed to load {dataset_name}: {e}")
            else:
                st.warning(f"‚ö†Ô∏è {filename} not found")
            
            progress_bar.progress((i + 1) / total_files)
        
        progress_bar.empty()
        st.session_state.loaded_datasets = datasets
        return True
        
    except Exception as e:
        st.error(f"Failed to load datasets: {e}")
        return False

def show_dataset_preview(dataset_name: str, df):
    """Show a comprehensive preview of the dataset"""
    st.subheader(f"üìä {dataset_name.title()} Dataset")
    
    # Metrics row
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Rows", f"{df.count():,}")
    
    with col2:
        st.metric("Total Columns", len(df.columns))
    
    with col3:
        # Calculate null percentage
        total_cells = df.count() * len(df.columns)
        null_count = sum([df.filter(df[col].isNull()).count() for col in df.columns])
        null_percentage = (null_count / total_cells * 100) if total_cells > 0 else 0
        st.metric("Null Values", f"{null_percentage:.1f}%")
    
    with col4:
        # Memory usage estimation
        memory_mb = df.count() * len(df.columns) * 8 / (1024 * 1024)  # Rough estimate
        st.metric("Est. Memory", f"{memory_mb:.1f} MB")
    
    # Schema information
    st.write("### üèóÔ∏è Schema Information")
    schema_data = []
    for field in df.schema.fields:
        # Get sample values for each column
        sample_values = df.select(field.name).limit(3).rdd.map(lambda x: x[0]).collect()
        sample_str = ", ".join([str(v) for v in sample_values if v is not None][:3])
        
        schema_data.append({
            "Column": field.name,
            "Type": str(field.dataType).replace("Type()", ""),
            "Nullable": "‚úÖ" if field.nullable else "‚ùå",
            "Sample Values": sample_str[:50] + "..." if len(sample_str) > 50 else sample_str
        })
    
    st.dataframe(pd.DataFrame(schema_data), use_container_width=True)
    
    # Data quality metrics
    st.write("### üìä Data Quality Metrics")
    quality_metrics = []
    
    for col_name in df.columns:
        total_count = df.count()
        null_count = df.filter(df[col_name].isNull()).count()
        distinct_count = df.select(col_name).distinct().count()
        
        quality_metrics.append({
            "Column": col_name,
            "Completeness": f"{((total_count - null_count) / total_count * 100):.1f}%",
            "Uniqueness": f"{(distinct_count / total_count * 100):.1f}%",
            "Null Count": null_count,
            "Distinct Values": distinct_count
        })
    
    st.dataframe(pd.DataFrame(quality_metrics), use_container_width=True)
    
    # Sample data with pagination
    st.write("### üëÄ Sample Data")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        sample_size = st.slider("Sample size", min_value=5, max_value=100, value=10)
    with col2:
        if st.button("üîÑ Refresh Sample"):
            st.rerun()
    
    sample_df = df.limit(sample_size).toPandas()
    st.dataframe(sample_df, use_container_width=True)

def create_transformation_interface():
    """Create interactive transformation selection interface"""
    st.header("üîß Data Transformations")
    
    if not st.session_state.loaded_datasets:
        st.warning("Please load datasets first before applying transformations.")
        return
    
    # Transformation pipeline builder
    st.subheader("üìã Build Transformation Pipeline")
    
    # Step-by-step pipeline builder
    current_step = st.session_state.current_step
    
    st.write(f"### Step {current_step}: Select Transformation")
    
    # Input dataset selection
    col1, col2 = st.columns(2)
    
    with col1:
        available_datasets = list(st.session_state.loaded_datasets.keys())
        if st.session_state.execution_results:
            # Add previously created datasets
            for result in st.session_state.execution_results:
                if result['output_name'] not in available_datasets:
                    available_datasets.append(result['output_name'])
        
        input_dataset = st.selectbox(
            "Select input dataset:",
            available_datasets,
            key=f"input_dataset_{current_step}"
        )
    
    with col2:
        transformation_categories = list(AVAILABLE_TRANSFORMATIONS.keys())
        selected_category = st.selectbox(
            "Select transformation category:",
            transformation_categories,
            key=f"category_{current_step}"
        )
    
    # Transformation selection
    if selected_category:
        transformations = AVAILABLE_TRANSFORMATIONS[selected_category]
        transformation_names = [t.__name__ for t in transformations]
        
        selected_transformation = st.selectbox(
            "Select transformation:",
            transformation_names,
            key=f"transformation_{current_step}"
        )
        
        if selected_transformation:
            # Get the transformation class
            transformation_class = next(
                t for t in transformations 
                if t.__name__ == selected_transformation
            )
            
            # Show transformation details
            st.write(f"**Description:** {transformation_class.__doc__ or 'No description available'}")
            
            # Parameter configuration
            st.write("### ‚öôÔ∏è Parameters")
            
            # Common parameters
            output_name = st.text_input(
                "Output dataset name:",
                value=f"{input_dataset}_{selected_transformation.lower()}",
                key=f"output_name_{current_step}"
            )
            
            # Transformation-specific parameters
            params = {}
            
            if "Filter" in selected_transformation:
                condition = st.text_input(
                    "Filter condition (e.g., age > 25):",
                    key=f"condition_{current_step}"
                )
                if condition:
                    params['condition'] = condition
            
            elif "Join" in selected_transformation:
                join_datasets = [d for d in available_datasets if d != input_dataset]
                if join_datasets:
                    right_dataset = st.selectbox(
                        "Dataset to join with:",
                        join_datasets,
                        key=f"right_dataset_{current_step}"
                    )
                    join_keys = st.text_input(
                        "Join keys (comma-separated):",
                        key=f"join_keys_{current_step}"
                    )
                    join_type = st.selectbox(
                        "Join type:",
                        ["inner", "outer", "left", "right"],
                        key=f"join_type_{current_step}"
                    )
                    params.update({
                        'right_dataset': right_dataset,
                        'join_keys': join_keys.split(',') if join_keys else [],
                        'join_type': join_type
                    })
            
            elif "Aggregate" in selected_transformation:
                if input_dataset in st.session_state.loaded_datasets:
                    df = st.session_state.loaded_datasets[input_dataset]
                    columns = df.columns
                    
                    group_columns = st.multiselect(
                        "Group by columns:",
                        columns,
                        key=f"group_cols_{current_step}"
                    )
                    
                    agg_column = st.selectbox(
                        "Column to aggregate:",
                        columns,
                        key=f"agg_col_{current_step}"
                    )
                    
                    agg_function = st.selectbox(
                        "Aggregation function:",
                        ["sum", "avg", "count", "max", "min"],
                        key=f"agg_func_{current_step}"
                    )
                    
                    params.update({
                        'group_columns': group_columns,
                        'agg_column': agg_column,
                        'agg_function': agg_function
                    })
            
            elif "Sort" in selected_transformation:
                if input_dataset in st.session_state.loaded_datasets:
                    df = st.session_state.loaded_datasets[input_dataset]
                    columns = df.columns
                    
                    sort_column = st.selectbox(
                        "Column to sort by:",
                        columns,
                        key=f"sort_col_{current_step}"
                    )
                    
                    ascending = st.checkbox(
                        "Ascending order",
                        value=True,
                        key=f"ascending_{current_step}"
                    )
                    
                    params.update({
                        'sort_column': sort_column,
                        'ascending': ascending
                    })
            
            # Execute transformation
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("‚ñ∂Ô∏è Execute Step", key=f"execute_{current_step}"):
                    execute_transformation_step(
                        input_dataset, transformation_class, output_name, params
                    )
            
            with col2:
                if st.button("‚ûï Add Step", key=f"add_step_{current_step}"):
                    st.session_state.current_step += 1
                    st.rerun()
            
            with col3:
                if st.button("üîÑ Reset Pipeline", key=f"reset_{current_step}"):
                    st.session_state.execution_results = []
                    st.session_state.current_step = 1
                    st.rerun()
    
    # Show execution history
    if st.session_state.execution_results:
        st.write("### üìù Execution History")
        
        history_data = []
        for i, result in enumerate(st.session_state.execution_results):
            history_data.append({
                "Step": i + 1,
                "Input": result['input_dataset'],
                "Transformation": result['transformation'],
                "Output": result['output_name'],
                "Rows": f"{result['output_rows']:,}",
                "Columns": result['output_cols'],
                "Status": "‚úÖ Success" if result.get('success', True) else "‚ùå Failed"
            })
        
        st.dataframe(pd.DataFrame(history_data), use_container_width=True)

def execute_transformation_step(input_dataset, transformation_class, output_name, params):
    """Execute a single transformation step"""
    try:
        with st.spinner(f"Executing {transformation_class.__name__}..."):
            # Get the input DataFrame
            if input_dataset in st.session_state.loaded_datasets:
                input_df = st.session_state.loaded_datasets[input_dataset]
            else:
                # Look for it in previous results
                for result in st.session_state.execution_results:
                    if result['output_name'] == input_dataset:
                        input_df = st.session_state.loaded_datasets[input_dataset]
                        break
                else:
                    st.error(f"Dataset {input_dataset} not found!")
                    return
            
            # Create transformation instance
            transformation = transformation_class()
            
            # Execute transformation
            output_df = transformation.transform(input_df, **params)
            
            # Store result
            st.session_state.loaded_datasets[output_name] = output_df
            
            # Record execution
            result = {
                'step': len(st.session_state.execution_results) + 1,
                'input_dataset': input_dataset,
                'transformation': transformation_class.__name__,
                'output_name': output_name,
                'output_rows': output_df.count(),
                'output_cols': len(output_df.columns),
                'parameters': params,
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
            
            st.session_state.execution_results.append(result)
            
            # Track lineage
            if st.session_state.spark_engine:
                st.session_state.spark_engine.lineage_tracker.add_transformation(
                    transformation_class.__name__,
                    input_dataset,
                    output_name,
                    params
                )
            
            st.success(f"‚úÖ Transformation completed! Created {output_name} with {result['output_rows']:,} rows")
            
    except Exception as e:
        st.error(f"‚ùå Transformation failed: {e}")
        
        # Record failed execution
        result = {
            'step': len(st.session_state.execution_results) + 1,
            'input_dataset': input_dataset,
            'transformation': transformation_class.__name__,
            'output_name': output_name,
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'success': False
        }
        st.session_state.execution_results.append(result)

def create_lineage_visualization():
    """Create interactive lineage visualization"""
    st.header("üåä Data Lineage Tracking")
    
    if not st.session_state.execution_results:
        st.info("No transformations executed yet. Please run some transformations to see lineage.")
        return
    
    # Create lineage graph
    G = nx.DiGraph()
    
    # Add nodes and edges based on execution results
    for result in st.session_state.execution_results:
        input_dataset = result['input_dataset']
        output_dataset = result['output_name']
        transformation = result['transformation']
        
        # Add nodes
        G.add_node(input_dataset, type='dataset', label=input_dataset)
        G.add_node(output_dataset, type='dataset', label=output_dataset)
        G.add_node(f"{transformation}_{result['step']}", type='transformation', label=transformation)
        
        # Add edges
        G.add_edge(input_dataset, f"{transformation}_{result['step']}")
        G.add_edge(f"{transformation}_{result['step']}", output_dataset)
    
    # Create plotly visualization
    pos = nx.spring_layout(G, k=3, iterations=50)
    
    # Separate nodes by type
    dataset_nodes = [node for node, data in G.nodes(data=True) if data.get('type') == 'dataset']
    transformation_nodes = [node for node, data in G.nodes(data=True) if data.get('type') == 'transformation']
    
    # Create traces
    edge_trace = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_trace.extend([
            go.Scatter(x=[x0, x1, None], y=[y0, y1, None], 
                      mode='lines', line=dict(width=2, color='#888'),
                      hoverinfo='none', showlegend=False)
        ])
    
    # Dataset nodes
    dataset_x = [pos[node][0] for node in dataset_nodes]
    dataset_y = [pos[node][1] for node in dataset_nodes]
    
    dataset_trace = go.Scatter(
        x=dataset_x, y=dataset_y,
        mode='markers+text',
        marker=dict(size=20, color='lightblue', line=dict(width=2)),
        text=dataset_nodes,
        textposition="middle center",
        name="Datasets",
        hovertemplate="<b>%{text}</b><br>Type: Dataset<extra></extra>"
    )
    
    # Transformation nodes
    transformation_x = [pos[node][0] for node in transformation_nodes]
    transformation_y = [pos[node][1] for node in transformation_nodes]
    
    transformation_trace = go.Scatter(
        x=transformation_x, y=transformation_y,
        mode='markers+text',
        marker=dict(size=15, color='orange', symbol='square'),
        text=[G.nodes[node]['label'] for node in transformation_nodes],
        textposition="middle center",
        name="Transformations",
        hovertemplate="<b>%{text}</b><br>Type: Transformation<extra></extra>"
    )
    
    # Create figure
    fig = go.Figure(data=edge_trace + [dataset_trace, transformation_trace])
    fig.update_layout(
        title="Data Lineage Graph",
        showlegend=True,
        hovermode='closest',
        margin=dict(b=20,l=5,r=5,t=40),
        annotations=[ dict(
            text="Interactive Data Lineage Visualization",
            showarrow=False,
            xref="paper", yref="paper",
            x=0.005, y=-0.002,
            xanchor='left', yanchor='bottom',
            font=dict(color='#888', size=12)
        )],
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Lineage summary table
    st.write("### üìã Lineage Summary")
    
    lineage_data = []
    for result in st.session_state.execution_results:
        lineage_data.append({
            "Step": result['step'],
            "Source": result['input_dataset'],
            "Transformation": result['transformation'],
            "Target": result['output_name'],
            "Rows In": "N/A",  # Could be calculated if needed
            "Rows Out": f"{result['output_rows']:,}",
            "Timestamp": result['timestamp'][:19].replace('T', ' ')
        })
    
    st.dataframe(pd.DataFrame(lineage_data), use_container_width=True)

def create_advanced_visualizations():
    """Create advanced pipeline visualizations"""
    st.header("üìà Advanced Visualizations")
    
    if not st.session_state.execution_results:
        st.info("No data to visualize. Please execute some transformations first.")
        return
    
    # Visualization options
    viz_type = st.selectbox(
        "Select visualization type:",
        ["Pipeline Flow", "Data Volume Changes", "Transformation Performance", "Data Quality Metrics"]
    )
    
    if viz_type == "Pipeline Flow":
        create_sankey_diagram()
    elif viz_type == "Data Volume Changes":
        create_volume_chart()
    elif viz_type == "Transformation Performance":
        create_performance_chart()
    elif viz_type == "Data Quality Metrics":
        create_quality_metrics()

def create_sankey_diagram():
    """Create Sankey diagram for data flow"""
    st.subheader("üåä Data Flow Sankey Diagram")
    
    # Prepare data for Sankey
    nodes = []
    links = []
    node_dict = {}
    
    # Collect all unique datasets and transformations
    all_entities = set()
    for result in st.session_state.execution_results:
        all_entities.add(result['input_dataset'])
        all_entities.add(result['output_name'])
        all_entities.add(result['transformation'])
    
    # Create node mapping
    for i, entity in enumerate(all_entities):
        node_dict[entity] = i
        nodes.append(entity)
    
    # Create links
    for result in st.session_state.execution_results:
        # Input -> Transformation
        links.append({
            'source': node_dict[result['input_dataset']],
            'target': node_dict[result['transformation']],
            'value': result['output_rows']
        })
        
        # Transformation -> Output
        links.append({
            'source': node_dict[result['transformation']],
            'target': node_dict[result['output_name']],
            'value': result['output_rows']
        })
    
    # Create Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=nodes,
            color="lightblue"
        ),
        link=dict(
            source=[link['source'] for link in links],
            target=[link['target'] for link in links],
            value=[link['value'] for link in links]
        )
    )])
    
    fig.update_layout(title_text="Data Flow Sankey Diagram", font_size=10)
    st.plotly_chart(fig, use_container_width=True)

def create_volume_chart():
    """Create data volume changes chart"""
    st.subheader("üìä Data Volume Changes")
    
    steps = []
    input_volumes = []
    output_volumes = []
    transformations = []
    
    for result in st.session_state.execution_results:
        steps.append(f"Step {result['step']}")
        output_volumes.append(result['output_rows'])
        transformations.append(result['transformation'])
        
        # For input volumes, we need to look up the previous step or original dataset
        # This is simplified - in a real implementation, you'd track this properly
        if result['step'] == 1:
            input_volumes.append(result['output_rows'])  # Approximate
        else:
            input_volumes.append(st.session_state.execution_results[result['step']-2]['output_rows'])
    
    fig = go.Figure()
    
    fig.add_trace(go.Bar(
        name='Input Rows',
        x=steps,
        y=input_volumes,
        marker_color='lightblue'
    ))
    
    fig.add_trace(go.Bar(
        name='Output Rows',
        x=steps,
        y=output_volumes,
        marker_color='darkblue'
    ))
    
    fig.update_layout(
        title='Data Volume Changes Through Pipeline',
        xaxis_title='Pipeline Steps',
        yaxis_title='Number of Rows',
        barmode='group'
    )
    
    st.plotly_chart(fig, use_container_width=True)

def create_performance_chart():
    """Create transformation performance metrics"""
    st.subheader("‚ö° Transformation Performance")
    st.info("Performance metrics would be available with execution timing data.")
    
    # Placeholder for performance visualization
    # In a real implementation, you'd track execution times
    performance_data = []
    for result in st.session_state.execution_results:
        performance_data.append({
            'Transformation': result['transformation'],
            'Step': result['step'],
            'Output Rows': result['output_rows'],
            'Complexity': 'Medium'  # Placeholder
        })
    
    if performance_data:
        st.dataframe(pd.DataFrame(performance_data), use_container_width=True)

def create_quality_metrics():
    """Create data quality metrics visualization"""
    st.subheader("üéØ Data Quality Metrics")
    
    if not st.session_state.loaded_datasets:
        st.info("No datasets loaded for quality analysis.")
        return
    
    quality_data = []
    
    for dataset_name, df in st.session_state.loaded_datasets.items():
        total_rows = df.count()
        total_cols = len(df.columns)
        
        # Calculate completeness
        null_counts = []
        for col in df.columns:
            null_count = df.filter(df[col].isNull()).count()
            null_counts.append(null_count)
        
        total_nulls = sum(null_counts)
        completeness = ((total_rows * total_cols - total_nulls) / (total_rows * total_cols)) * 100
        
        quality_data.append({
            'Dataset': dataset_name,
            'Rows': total_rows,
            'Columns': total_cols,
            'Completeness': f"{completeness:.1f}%",
            'Total Nulls': total_nulls
        })
    
    if quality_data:
        df_quality = pd.DataFrame(quality_data)
        
        # Create quality metrics chart
        fig = px.bar(
            df_quality, 
            x='Dataset', 
            y='Rows',
            title='Dataset Size Comparison',
            color='Completeness'
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Quality metrics table
        st.dataframe(df_quality, use_container_width=True)

def sidebar_controls():
    """Create sidebar controls"""
    st.sidebar.title("üéõÔ∏è Pipeline Control")
    
    # Data Management Section
    st.sidebar.header("üìä Data Management")
    
    # Check if sample data exists
    data_dir = os.path.join(project_root, 'data')
    customers_file = os.path.join(data_dir, 'customers.csv')
    
    if not os.path.exists(customers_file):
        st.sidebar.warning("‚ö†Ô∏è Sample data not found!")
        if st.sidebar.button("üé≤ Generate Sample Data"):
            with st.spinner("Generating sample data..."):
                try:
                    from data.generate_sample_data import save_sample_data
                    save_sample_data()
                    st.sidebar.success("‚úÖ Sample data generated!")
                    st.rerun()
                except Exception as e:
                    st.sidebar.warning(f"Main generator failed: {e}")
                    st.sidebar.info("üîß Using fallback generator...")
                    if generate_fallback_data():
                        st.sidebar.success("‚úÖ Fallback data generated!")
                        st.rerun()
                    else:
                        st.sidebar.error("‚ùå All generators failed")
    else:
        st.sidebar.success("‚úÖ Sample data available!")
        
        if st.sidebar.button("üì• Load Datasets"):
            if load_sample_data():
                st.sidebar.success("‚úÖ Datasets loaded!")
                st.rerun()
    
    # Spark Session Management
    st.sidebar.header("‚ö° Spark Session")
    
    if st.session_state.spark_engine is None:
        if st.sidebar.button("üöÄ Initialize Spark"):
            initialize_spark()
            st.rerun()
    else:
        st.sidebar.success("‚úÖ Spark session active")
        if st.sidebar.button("üîÑ Restart Spark"):
            st.session_state.spark_engine = None
            st.rerun()
    
    # Pipeline Status
    st.sidebar.header("üìã Pipeline Status")
    
    st.sidebar.metric("Datasets Loaded", len(st.session_state.loaded_datasets))
    st.sidebar.metric("Transformations", len(st.session_state.execution_results))
    st.sidebar.metric("Current Step", st.session_state.current_step)
    
    # Quick Actions
    st.sidebar.header("‚ö° Quick Actions")
    
    if st.sidebar.button("üóëÔ∏è Clear All Data"):
        st.session_state.loaded_datasets = {}
        st.session_state.execution_results = []
        st.session_state.current_step = 1
        st.sidebar.success("‚úÖ Data cleared!")
        st.rerun()
    
    if st.sidebar.button("üíæ Export Results"):
        if st.session_state.execution_results:
            export_data = {
                'execution_results': st.session_state.execution_results,
                'timestamp': datetime.now().isoformat()
            }
            st.sidebar.download_button(
                "üì• Download JSON",
                data=json.dumps(export_data, indent=2),
                file_name=f"pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
        else:
            st.sidebar.warning("No results to export")

def main():
    """Main Streamlit application"""
    
    # Custom header
    st.markdown("""
    <div class="main-header">
        üîÑ Data Lineage Pipeline
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    <div style="text-align: center; margin-bottom: 2rem; color: #666;">
        <b>Complete Interactive Data Transformation & Lineage Tracking Platform</b><br>
        Transform data with PySpark ‚Ä¢ Track lineage in real-time ‚Ä¢ Visualize data flows ‚Ä¢ Generate production DAGs
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar controls
    sidebar_controls()
    
    # Main content area
    if len(st.session_state.loaded_datasets) == 0:
        # Welcome screen
        st.markdown("""
        ## üéØ Welcome to the Data Lineage Pipeline!
        
        This comprehensive platform allows you to:
        
        ### üöÄ Quick Start Guide:
        1. **üìä Generate Sample Data** ‚Üí Click "Generate Sample Data" in the sidebar
        2. **‚ö° Initialize Spark** ‚Üí Click "Initialize Spark" to start the engine  
        3. **üì• Load Datasets** ‚Üí Import your data into Spark DataFrames
        4. **üîß Build Pipeline** ‚Üí Use the transformation interface to create your data pipeline
        5. **üåä Track Lineage** ‚Üí Visualize how data flows through transformations
        6. **üìà Analyze Results** ‚Üí View advanced visualizations and metrics
        
        ### ‚ú® Key Features:
        - **15+ Transformation Types** across cleaning, aggregation, and joining operations
        - **Real-time Lineage Tracking** with interactive visualizations
        - **Data Quality Assessment** with comprehensive metrics
        - **Production-Ready Output** for Airflow deployment
        - **Interactive UI** with drag-and-drop pipeline building
        
        ### üéõÔ∏è Get Started:
        Use the sidebar controls to begin! The system will guide you through each step.
        """)
        
        # Show sample screenshots or demo
        st.info("üí° **Tip:** Start by generating sample e-commerce data to explore all features!")
        
    else:
        # Main application tabs
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìä Data Overview", 
            "üîß Transformations", 
            "üåä Lineage Tracking", 
            "üìà Visualizations"
        ])
        
        with tab1:
            st.header("üìä Data Overview")
            
            # Dataset selector
            selected_dataset = st.selectbox(
                "Select a dataset to explore:",
                options=list(st.session_state.loaded_datasets.keys()),
                index=0
            )
            
            if selected_dataset:
                show_dataset_preview(selected_dataset, st.session_state.loaded_datasets[selected_dataset])
        
        with tab2:
            create_transformation_interface()
        
        with tab3:
            create_lineage_visualization()
        
        with tab4:
            create_advanced_visualizations()

if __name__ == "__main__":
    main()
